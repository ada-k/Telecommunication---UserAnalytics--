# -*- coding: utf-8 -*-
"""kernel250a111cd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jwIp1hiR9HMZp-7G1MVr8piXyuwdg4By

### Table of Contents
* [Libraries](#Libraries)
* [Dataset](#Dataset)
* [Null Values](#Null-Values)
* [Preprocessing](#PreProcessing)
* [1. User Overiew Analysis](#1.-User-Overiew-Analysis)
    * [User Aggregates](#Aggregate-per-user-the-following-information-in-the-column:)
    * [Non-Graphical Univariate Analysis](#Non-Graphical-Univariate-Analysis)
    * [Graphical Univariate Analysis.](#Graphical-Univariate-Analysis.)
    * [Bivariate Analysis](#Bivariate-Analysis)
    * [Variable Transformation](#Variable-transformations)
    * [Correlation Analysis](#Correlation-Analysis)
    * [Dimensionality Reduction](#Dimensionality-Reduction)
* [2. User Engagement Analysis](#2.-User-Engagement-analysis)
    * [Top 10 customers per metric.](#Top-10-users-with-high-sessions-frequency)
    * [Normalization and Kmeans](#Normalize-each-engagement-metric-and-run-a-k-means-(k=3)-to-classify-customers-in-three-groups-of-engagement.)
    * [Most engaged users per app](#*-Aggregate-user-total-traffic-per-application-and-derive-the-top-10-most-engaged-users-per-application)
    * [Most used apps](#*-Plot-the-top-3-most-used-applications.)
    * [KMeans](#*-Using-k-means-clustering-algorithm,-group-users-in-k-engagement-clusters-based-on-the-engagement-metrics:)
* [3. User Experience Analysis](#3.-Experience-Analytics)
    * [Top 10, Bottom 10 and Frequent 10](#Compute-&-list-10-of-the-top,-bottom-and-most-frequent:)
    * [Throughput Distribution and Average TCP](#Compute-&-report:)
    * [KMeans](#Using-the-experience-metrics-above,-perform-a-k-means-clustering-(where-k-=-3)-to-segment-users-into-groups-of-experiences-and-provide-a-brief-description-of-each-cluster.)
* [4. User Satisfaction Analysis](#1.-User-Overiew-Analysis)
    * [Engagement and Experience scores](#Write-a-python-program-to-assign:)
    * [Satisfaction Score](#Merge-the-engagement-and-experience-datasets)
    * [Regression Model.](#Run-a-regression-model-of-your-choice-to-predict-the-satisfaction-score-of-a-customer.)
    * [Kmeans](#Run-a-k-means-(k=2)-on-the-engagement-&-the-experience-score.)
    * [Cluster Scores](#Aggregate-the-average-engagement-&-experience-score-per-cluster.)
    * [Correlation Analysis](#Correlation-Analysis)
    * [MySQL](#Export-your-final-table-containing-all-user-id-+-engagement,-experience-&-satisfaction-scores-in-your-local-MySQL-database.-Report-a-screenshot-of-a-select-on-the-exported-table.)

## Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from scipy import stats
from scipy.stats import skew, norm
from scipy.special import boxcox1p
from scipy.stats import boxcox_normmax
from sklearn.cluster import KMeans
import scipy.spatial.distance as dist
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor


import warnings
warnings.filterwarnings(action="ignore")

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""## Dataset"""

raw = pd.read_excel('/kaggle/input/Week2_challenge_data_source.xlsx')
raw.head()

raw.shape

"""Quite a bulk dataset.

## PreProcessing

### Numerical, Categorical and TimeStamp features
"""

numerical = []
categorical = []
timestamps = []

for col in raw.columns:
    if raw[col].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:
        numerical.append(col)
    elif raw[col].dtype == object:
        categorical.append(col)
    else:
        timestamps.append(col)
        
        
features = numerical+categorical+timestamps
data = raw[features]
data.head(3)

data.dtypes

data.shape

"""### Null Values

#### Numerical Cols
"""

#Null numerical values (percentage)
null = data[numerical].isna().sum().sort_values(ascending = False)
null_per = (null/150001) * 100
null_perc = pd.DataFrame(null_per)
null_perc

"""Some columns have big percentages of missing values. For the meantime, imputation with column means will be done incase the columns come in handy. Otherwise, they'll be dropped. The **big_data** list below will store the names of cols with missing values > 50%"""

# columns with >50% null values
bad_data = []
for entry, column in zip(null_perc.iloc[:, 0], null_perc.index):
    if entry >= 50:
        bad_data.append(column)
        
# bad_data

"""All the numerical columns with missing values will be imputed with the mean of the column given the absence of nominal columns."""

# get the numerical null values indices from the dataframe
a = null_perc.index.to_list()
# col_index = pd.DataFrame(raw.columns, columns = ['name'])
indices = []
for col in a:
    k = data.columns.get_loc(col)
    indices.append(k)
# col_index
len(indices)

# MSISDN/Number is more of a categorical column than numurical given its a unique id for each user.
# imputing it with mean would result to the creation of non-existence users. Like other categorical variables with nan, it'll be imputed with mode
data.columns.get_loc('MSISDN/Number')

indices.remove(4)
len(indices)

#imputing with column means.
columns = indices
for col in columns:
    x = data.iloc[:, col].values
    x = x.reshape(-1,1)
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
    imputer = imputer.fit(x)
    x = imputer.transform(x)
    data.iloc[:, col] = x

data[numerical].isna().any()

"""#### Categorical and Timestamp cols"""

#Null categorical and timestamps values 
null = data[categorical + timestamps].isna().sum().sort_values(ascending = False)
null = (null/150001) * 100
null = pd.DataFrame(null)
null

"""The null values in these columns contribute a small percentage to the column values, thus not  a big deal. Imputation will be done using the mode/most_frequent value. But first, they have to be converted to type **str**."""

for col in categorical+timestamps:
    data[col] = data[col].astype(str)

# get the numerical null values indices from the dataframe
a = null.index.to_list()
# col_index = pd.DataFrame(raw.columns, columns = ['name'])
indices = []
for col in a:
    k = data.columns.get_loc(col)
    indices.append(k)
    
indices

# imputing with mode
indices.append(4) #the user id col
columns = indices
for col in columns:
    x = data.iloc[:, col].values
    x = x.reshape(-1,1)
    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
    imputer = imputer.fit(x)
    x = imputer.transform(x)
    data.iloc[:, col] = x

data[categorical+timestamps].isna().any()

"""All null values taken care of.

### Timestamps to Pandas Datetime object
For imputation to work, their data type had to be change to string. That has to be reversed.
"""

data['Start'] = pd.to_datetime(data['Start'])
data['End'] = pd.to_datetime(data['End'])

"""### Feature Generation"""

data['total_bytes'] =  data['Total UL (Bytes)'] + data['Total DL (Bytes)']
data['social_media'] = data['Social Media DL (Bytes)'] + data['Social Media UL (Bytes)']
data['email'] = data['Email DL (Bytes)'] + data['Email UL (Bytes)']
data['google'] = data['Google DL (Bytes)'] + data['Google UL (Bytes)']
data['youtube'] = data['Youtube DL (Bytes)'] + data['Youtube UL (Bytes)']
data['netflix'] = data['Netflix UL (Bytes)'] + data['Netflix DL (Bytes)']
data['gaming'] = data['Gaming DL (Bytes)'] + data['Gaming UL (Bytes)']
data['other'] = data['Other DL (Bytes)'] + data['Other UL (Bytes)']
data['total_tp'] = data['Avg Bearer TP DL (kbps)'] + data['Avg Bearer TP UL (kbps)']
data['total_rtt'] = data['Avg RTT DL (ms)'] + data['Avg RTT UL (ms)']
data['total_tcp'] = data['TCP DL Retrans. Vol (Bytes)'] + data['TCP UL Retrans. Vol (Bytes)']

"""### Encoding Categorical columns

The method pd.get_dummies() encodes categorical variables using the OneHotEncoder method which is preferred over the LabelEncoder. The latter assigns numerical valuesm to the variable's values hierarchically. This will result to biased data since some entries could be perceived as having more weight than others when the only difference is class type.
Despite the preference of OHE  over LE, the dataset is already big, encoding it with OHE would increase it's size at least twice. Due to computational limitations (memory allocation), LE is used. But always, OHE is a better performer.
"""

# enc_data = pd.get_dummies(data).reset_index(drop=True)
# enc_data.shape
data[categorical].head(3)

columns = [50, 52, 51, 49]
for col in columns:
    x = data.iloc[:, col].values
    x = x.reshape(-1,1)
    encoder = LabelEncoder()
    encoder = encoder.fit(x)
    x = encoder.transform(x)
    name = data.columns[col]
    data[name + '_encoded'] = x   # create new columns with the encoded values instead of replacing them, might come in handy at later stages

"""## 1. User Overiew Analysis

### Top 10 handsets
"""

top_10_sets = data['Handset Type'].value_counts(ascending = False).head(10)
top_10_sets = pd.DataFrame(top_10_sets)
top_10_sets

plt.figure(figsize=(20,10))
sns.barplot(y = top_10_sets.index, x = top_10_sets['Handset Type'])
plt.xlabel('Number of handsets')
plt.title('A barplot indicating top ten handset types')

"""### Bottom 10 handsets"""

b_10 = data['Handset Type'].value_counts(ascending = False).tail(10)
b_10 = pd.DataFrame(b_10)

plt.figure(figsize=(20,10))
sns.barplot(y = b_10.index, x = b_10['Handset Type'])
plt.xlabel('Number of handsets')
plt.title('A barplot indicating bottom ten handset types')

"""### top 3 handset manufacturers"""

top_3_manufacturers = data['Handset Manufacturer'].value_counts(ascending = False).head(3)
top_3_manufacturers = pd.DataFrame(top_3_manufacturers)
top_3_manufacturers

plt.figure(figsize=(10,5))
sns.barplot(y = top_3_manufacturers.index, x = top_3_manufacturers['Handset Manufacturer'])
plt.xlabel('Number of products')
plt.title('A barplot indicating top 3 handset manufacturers')

"""### Bottom 3 manufacturers"""

b3 = data['Handset Manufacturer'].value_counts(ascending = False).tail(3)
b3 = pd.DataFrame(b3)

plt.figure(figsize=(10,5))
sns.barplot(y = b3.index, x = b3['Handset Manufacturer'])
plt.xlabel('Number of products')
plt.title('A barplot indicating bottom 3 handset manufacturers')

"""###  top 5 handsets per handset manufacturer"""

a = data.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name = 'count').sort_values(['Handset Manufacturer', 'count'], ascending = False).groupby('Handset Manufacturer').head(5)
a.tail(20)

# data.groupby(["Handset Manufacturer", 'Handset Type'])['Handset Type'].count().reset_index(name='Count')
# b = pd.DataFrame(data.groupby(['Handset Manufacturer', 'Handset Type']).size()).groupby('Handset Manufacturer')
# b = b.apply(lambda _df: _df.sort_values(by=['Handset Manufacturer'], ascending = False))
# b.head(20)
# data[data['Handset Type'] == 'Apple iPhone 7 (A1778)'].shape

# data.groupby('Handset Manufacturer')['Handset Type'].value_counts().unstack(0).plot.barh()

"""### Make a short interpretation and recommendation to marketing teams

* Apple, Huawei and Samsung are the leading manufacturers. The top 10 handsets also are manufactured by these companies. Some of the bottom 10 handsets also belong to these manufacturers. Thus the volume of the most common handsets should be increased to replace the least common sets. 
* Products from the bottom 3 manufacturers should no longer be stocked. Their extremely small volume matches small profit percentage in comparison to other manufacturers.

### Aggregate per user the following information in the column:

1.  number of xDR sessions
"""

xdr_sess = data[['Bearer Id', 'MSISDN/Number']]
a = xdr_sess.groupby('MSISDN/Number').count()
a.columns = ['xdr sessions']
a.head()

t = a.sort_values(by = 'xdr sessions', ascending = False).head(10)
plt.figure(figsize=(8,6))
sns.barplot(x = t.index, y = t['xdr sessions'])
plt.xticks(rotation = 45)
plt.title('users with highest session numbers')

"""2.  Session duration"""

duration = data[['Dur. (ms)', 'MSISDN/Number']]
b = duration.groupby('MSISDN/Number').sum()
b.columns = ['sess_duration (ms)']
b.head()

t = b.sort_values(by = 'sess_duration (ms)', ascending = False).head(10)
plt.figure(figsize=(8,6))
sns.barplot(x = t.index, y = t['sess_duration (ms)'])
plt.xticks(rotation = 45)
plt.title('users with longest session duration')

"""3. The total download (DL) and upload (UL) data"""

byte = data[['MSISDN/Number', 'Total UL (Bytes)', 'Total DL (Bytes)', 'total_bytes']]
c = byte.groupby('MSISDN/Number').sum()
c.head()

# plotting a sample of the data
t = c.sample(10)
pos = list(range(len(t['Total UL (Bytes)'])))
width = 0.25
fig, ax = plt.subplots(figsize=(10,12))
plt.bar(pos, t['Total UL (Bytes)'], width, alpha=0.5, color='b')
plt.bar([p + width for p in pos], t['Total DL (Bytes)'], width, alpha=0.5, color='r')
plt.bar([p + width*2 for p in pos], t['total_bytes'], width, alpha=0.5, color='g')
ax.set_ylabel('bytes')
ax.set_title('Bytes data per user')
ax.set_xticks([p + 1.5 * width for p in pos])
ax.set_xticklabels(c.index)
plt.xticks(rotation = 45)

plt.xlim(min(pos)-width, max(pos)+width*4)
plt.ylim([0, max(t['Total UL (Bytes)'] + t['Total DL (Bytes)'] + t['total_bytes'])] )
plt.legend(['Upload Bytes', 'Download Bytes', 'Total Bytes'], loc='upper left')
plt.grid()
plt.title('A sample of users upload, download and total bytes')

"""1. Upload bytes are less than download bytes throughout.

4. The total data volume (in Bytes) during this session for each application
"""

# I had already created total bytes columns for all the applications.
apps = data[['MSISDN/Number','social_media', 'google', 'email', 'youtube', 'netflix', 'gaming', 'other']]
d = apps.groupby('MSISDN/Number').sum()
d.head()

# sns.barplot(x=d.index.head(), y=, hue='variable', data=df1)
# plt.xticks(rotation=90)
# plt.ylabel('Returns')
# plt.title('Portfolio vs Benchmark Returns');

"""## Non-Graphical Univariate Analysis

Variables of interest: Number of xDR sessions,  Session duration,  the total download (DL) and upload (UL) data , the total data volume (in Bytes) during this session for each application (Social Media, Google, Email, YouTube, Netflix, Gaming).

### Dispersion and Central Tendency
"""

# data.describe()

# variables = data[['Bearer Id', 'Dur. (ms).1', 'Total UL (Bytes)', 'Total DL (Bytes)', 'social_media', 'netflix', 'gaming', 'youtube', 'google', 'email', 'other']]
# variables.describe()

variables = data[['Bearer Id', 'Dur. (ms).1', 'Total UL (Bytes)', 'Total DL (Bytes)', 'social_media', 'netflix',
                  'gaming', 'youtube', 'google', 'email', 'other', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)',
                 'Avg Bearer TP UL (kbps)', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']]
variables.describe().transpose()

data['Bearer Id'].nunique()

# skewness and kurtosis
s = variables.skew(axis = 0, skipna = True) 
k = variables.kurt(axis = 0, skipna = True) 
pd.DataFrame([s, k], index = ['skewness', 'kurtosis'])

"""Highly skewed distribution: If the skewness value is less than −1 or greater than +1.

Moderately skewed distribution: If the skewness value is between −1 and −½ or between +½ and +1.

Approximately symmetric distribution: If the skewness value is between −½ and +½.


Really Flat distribution: Kurtosis less than -1.
Peaked distribution: Kurtosis more than 1.

Summary of some of the variables:


1. **Bearer Id** ~ This is the session identifier, thus not quantitative. It has 134709 instances thus the recorded number of xDR sessions are 134709.
2. **Dur. (ms).1** ~ This is the total duration of an xDR session in micro seconds. Across the 134709 sessions, it has the following measures:
    * Average:          	1.046091e+08
    * Standard Deviation: 	8.103734e+07
    * Minimum Value:     	7.142988e+06 	
    * Lower Quartile    	5.744206e+07 	
    * Median            	8.639998e+07 	 
    * Upper Quartile     	1.324307e+08
    * Maximum Value:    	1.859336e+09 		
    
    
3. **Total UL (Bytes)** ~ Data volume in bytes sent during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	4.112121e+07
    * Standard Deviation: 	1.127635e+07  
    * Minimum Value:     	2.866892e+06 
    * Lower Quartile    	3.322203e+07 
    * Median            	4.558409e+08
    * Upper Quartile     	4.903424e+07 
    * Maximum Value:    	7.833131e+07 
    
    
4. **Total DL (Bytes)** ~ Data volume in bytes received during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	4.546434e+085
    * Standard Deviation: 	2.441421e+08 
    * Minimum Value:     	7.114041e+06 
    * Lower Quartile    	2.431072e+08 
    * Median            	1.826471e+06 
    * Upper Quartile     	6.657051e+08 
    * Maximum Value:    	9.029696e+08  
    
    
5. **social_media** ~ Total social media data volume sent and received by the MS during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	1.828250e+06
    * Standard Deviation: 	1.035646e+06
    * Minimum Value:     	1.563000e+03
    * Lower Quartile    	9.322180e+05
    * Median            	8.639900e+04
    * Upper Quartile     	2.727487e+06 
    * Maximum Value:    	3.650861e+06 
   
   
   
6. **netflix** ~ Total netflix data volume sent and received by the MS during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	2.262861e+07 
    * Standard Deviation: 	9.260820e+06 
    * Minimum Value:     	9.843200e+04 
    * Lower Quartile    	1.597946e+07
    * Median            	2.263554e+07 
    * Upper Quartile     	2.929044e+07 
    * Maximum Value:    	4.519815e+07 
    
    
7. **gaming** ~ Total gaming data volume sent and received by the MS during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	4.303331e+08 
    * Standard Deviation: 	2.440199e+08 
    * Minimum Value:     	3.063580e+05
    * Lower Quartile    	2.187279e+08
    * Median            	4.316150e+08 
    * Upper Quartile     	6.414159e+08 
    * Maximum Value:    	8.592028e+08 
    
    
8. **youtube)** ~ Total youtube data volume sent and received by the MS during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	2.264348e+07 
    * Standard Deviation: 	9.246800e+06 
    * Minimum Value:     	7.890300e+04
    * Lower Quartile    	1.599846e+07 
    * Median            	2.266177e+07
    * Upper Quartile     	2.929260e+07
    * Maximum Value:    	4.519008e+07 
    
    
9. **email** ~ Total email data volume sent and received by the MS during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	2.259102e+06 
    * Standard Deviation: 	1.071109e+06 
    * Minimum Value:     	8.359000e+03 
    * Lower Quartile    	1.359344e+06
    * Median            	2.263567e+06
    * Upper Quartile     	3.159818e+06 
    * Maximum Value:    	4.518036e+06
    
    
10. **google** ~ Total google data volume sent and received by the MS during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	7.807295e+06 
    * Standard Deviation: 	3.516420e+06 
    * Minimum Value:     	4.033000e+04 
    * Lower Quartile    	4.943599e+06 
    * Median            	7.812835e+06
    * Upper Quartile     	1.068280e+07 
    * Maximum Value:    	1.552878e+07 
    
    
    
11. **other** ~ Total Other data volume sent and received by the MS during this session. Across the 134709 sessions, it has the following measures:
    * Average:          	4.293653e+08
    * Standard Deviation: 	2.432681e+08
    * Minimum Value:     	1.490450e+05
    * Lower Quartile    	2.185534e+08
    * Median            	4.299865e+08
    * Upper Quartile     	6.399275e+08
    * Maximum Value:    	8.595209e+08

### Outlier Detection
"""

# Outlier detection

# chauvenets creterion (assumes normally distributed data)

# def chauvenet(array):
#     mean = array.mean()           # Mean of incoming array
#     stdv = array.std()            # Standard deviation
#     N = len(array)                # Lenght of incoming array
#     criterion = 1.0/(2*N)         # Chauvenet's criterion
#     d = abs(array-mean)/stdv      # Distance of a value to mean in stdv's
#     prob = erfc(d)                # Area normal dist.    
#     return prob < criterion       # Use boolean array outside this function#

#zscore (assumes normal distribution too. not good)

#IQR

#google
Q1 = np.percentile(data.google, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.google, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.google, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.google, interpolation = 'midpoint') 
o = (data.google < (Q1 - 1.5 * IQR)) |(data.google > (Q3 + 1.5 * IQR))
i = o.unique()

#youtube
Q1 = np.percentile(data.youtube, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.youtube, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.youtube, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.youtube, interpolation = 'midpoint') 
o = (data.youtube < (Q1 - 1.5 * IQR)) |(data.youtube > (Q3 + 1.5 * IQR))
j = o.unique()

#gaming
Q1 = np.percentile(data.gaming, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.gaming, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.gaming, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.gaming, interpolation = 'midpoint') 
o = (data.gaming < (Q1 - 1.5 * IQR)) |(data.gaming > (Q3 + 1.5 * IQR))
k = o.unique()


#netflix
Q1 = np.percentile(data.netflix, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.netflix, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.netflix, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.netflix, interpolation = 'midpoint') 
o = (data.netflix < (Q1 - 1.5 * IQR)) |(data.netflix > (Q3 + 1.5 * IQR))
l = o.unique()


#email
Q1 = np.percentile(data.email, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.email, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.email, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.email, interpolation = 'midpoint') 
o = (data.email < (Q1 - 1.5 * IQR)) |(data.email > (Q3 + 1.5 * IQR))
m = np.unique(o)


#social_media
Q1 = np.percentile(data.social_media, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.social_media, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.social_media, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.social_media, interpolation = 'midpoint') 
o = (data.social_media < (Q1 - 1.5 * IQR)) |(data.social_media > (Q3 + 1.5 * IQR))
n = o.unique()

#others
Q1 = np.percentile(data.other, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.other, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.other, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.other, interpolation = 'midpoint') 
o = (data.other < (Q1 - 1.5 * IQR)) |(data.other > (Q3 + 1.5 * IQR))
p = o.unique()




['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)',
                 'Avg Bearer TP UL (kbps)', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']
#Dur. (ms).1
Q1 = np.percentile(data['Dur. (ms).1'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(data['Dur. (ms).1'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(data['Dur. (ms).1'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(data['Dur. (ms).1'], interpolation = 'midpoint') 
o = (data['Dur. (ms).1'] < (Q1 - 1.5 * IQR)) |(data['Dur. (ms).1'] > (Q3 + 1.5 * IQR))
q = o.unique()

#total_bytes
Q1 = np.percentile(data.total_bytes, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.total_bytes, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.total_bytes, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.total_bytes, interpolation = 'midpoint') 
o = (data.total_bytes < (Q1 - 1.5 * IQR)) |(data.total_bytes > (Q3 + 1.5 * IQR))
r = o.unique()

#total_tp
Q1 = np.percentile(data.total_tp, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.total_tp, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.total_tp, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.total_tp, interpolation = 'midpoint') 
o = (data.total_tp < (Q1 - 1.5 * IQR)) |(data.total_tp > (Q3 + 1.5 * IQR))
s = o.unique()

#total_rtt
Q1 = np.percentile(data.total_rtt, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.total_rtt, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.total_rtt, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.total_rtt, interpolation = 'midpoint') 
o = (data.total_rtt < (Q1 - 1.5 * IQR)) |(data.total_rtt > (Q3 + 1.5 * IQR))
t = o.unique()

#total_tcp
Q1 = np.percentile(data.total_tcp, 25, interpolation = 'midpoint')  
Q2 = np.percentile(data.total_tcp, 50, interpolation = 'midpoint')  
Q3 = np.percentile(data.total_tcp, 75, interpolation = 'midpoint')  
IQR = stats.iqr(data.total_tcp, interpolation = 'midpoint') 
o = (data.total_tcp < (Q1 - 1.5 * IQR)) |(data.total_tcp > (Q3 + 1.5 * IQR))
u = o.unique()


print('google', i)
print('youtube', j)
print('gaming', k)
print('netflix', l)
print('email', m)
print('social media', n)
print('other', p)
print('Duration', q)
print('total bytes', r)
print('total_tp', s)
print('total_rtt', t)
print('total_tcp', u)

"""No outliers present in the apps.
Outliers present in the **session duration, total throughput, total rtt*** and ***total tcp**. This will be taken care of in a later section.

Same method will be extended to other columns.

## Graphical Univariate Analysis.

Variables of interest: Number of xDR sessions, Session duration, the total download (DL) and upload (UL) data , the total data volume (in Bytes) during this session for each application (Social Media, Google, Email, YouTube, Netflix, Gaming).

For graphical analysis of univariate categorical data, histograms are typically used. The histogram represents the frequency (count) or proportion (count/total count) of cases for a range of values. Typically, between about 5 and 30 bins are chosen. Histograms are one of the best ways to quickly learn a lot about your data, including central tendency, spread, modality, shape and outliers. Stem and Leaf plots could also be used for the same purpose. Boxplots can also be used to present  information about the central tendency, symmetry and skew, as well as outliers.  Quantile normal plots or QQ plots and other techniques could also be used here.

Number of xDR sessions,  Session duration,  the total download (DL) and upload (UL) data , the total data volume (in Bytes) during this session for each application (Social Media, Google, Email, YouTube, Netflix, Gaming).
"""

sns.distplot(data['gaming'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['gaming'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['gaming'], plot=plt)
plt.show()

# sns.distplot(data['Social Media DL (Bytes)']) # numeric
sns.distplot(data['netflix'] , bins = 10, fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['netflix'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['netflix'], plot=plt)
plt.show()

# sns.distplot(data['Social Media DL (Bytes)']) # numeric
sns.distplot(data['youtube'] , fit=norm);
# sns.violinplot(data['youtube'])

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['youtube'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['youtube'], plot=plt)
plt.show()

# sns.distplot(data['email'], fit = norm) # numeric
sns.boxplot(data['email'])

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['email'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['email'], plot=plt)
plt.show()

# sns.distplot(data['Social Media DL (Bytes)']) # numeric
# sns.distplot(data['google'] , fit=norm);
sns.boxplot(data['google'])

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['google'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['google'], plot=plt)
plt.show()

# sns.distplot(data['Social Media DL (Bytes)']) # numeric
sns.distplot(data['social_media'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['social_media'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['social_media'], plot=plt)
plt.show()

# total upload and download bytes
sns.distplot(data['total_bytes'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['total_bytes'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['total_bytes'], plot=plt)
plt.show()

# xdr ms
sns.distplot(data['Dur. (ms).1'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['Dur. (ms).1'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['Dur. (ms).1'], plot=plt)
plt.show()

"""Heavily rightly skewed."""

# total_tp
sns.distplot(data['total_tp'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['total_tp'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['total_tp'], plot=plt)
plt.show()

"""Heavily skewed to the right."""

# total_rtt
sns.distplot(data['total_rtt'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['total_rtt'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['total_rtt'], plot=plt)
plt.show()

"""Right tailed too."""

# total_tcp
sns.distplot(data['total_tcp'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(data['total_tcp'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(data['total_tcp'], plot=plt)
plt.show()

"""## Bivariate Analysis
– explore the relationship between each application & the total DL+UL data using appropriate methods and interpret your findings.

(Social Media, Google, Email, YouTube, Netflix, Gaming ++ total_bytes).
"""

# data['total'] = data['Total DL (Bytes)'] + data['Total UL (Bytes)']
# sns.barplot(x = data['social_media'], y = data['total'])

biv_data = data[['social_media', 'google', 'email', 'youtube', 'netflix', 'gaming', 'total_bytes', 'other']]
# adding a summation row in all columns
biv_data.loc['totals'] = biv_data.sum()
biv_data.tail()

"""### Applications total bytes"""

a = biv_data.tail(1)
long_df = pd.melt(a)
long_df = long_df.drop([6], axis  = 0)
plt.figure(figsize=(12,6))
sns.barplot(x = long_df.variable, y = long_df.value)
plt.title('Applications UL and DL data')
plt.ylabel('total bytes')
plt.xlabel('application')

fig = plt.figure(figsize =(10, 7)) 
a = biv_data.drop(['total_bytes'], axis = 1)
plt.pie(a.loc['totals'], labels = a.columns)
plt.title('A pie chart showing total data bytes for various applications')
plt.show()

"""Will you look at that!!!
Gamers Republic!!
*  The large percentage of **others** can be explained by many applications under the name others.
*  As for gaming:
  1. The applications are heavy. (Require more data)
  2. Most of the users are gamers.
  3. Most of the applications in the hand sets are games.

#### One on one plots on the apps vs total_bytes
"""

plt.figure(figsize=(6, 4))
sns.scatterplot(data = data, x='email', y='total_bytes')
plt.title('Scatter plot showing relationship between total_bytes and email')
plt.show()

plt.figure(figsize=(6, 4))
# sns.jointplot(data = data, x='google', y='total_bytes', kind = 'hex', color = '#4CB391')
sns.scatterplot(data = data, x='google', y='total_bytes', color = '#4CB391')
# plt.title('Hexplot plot showing relationship between total_bytes and google', size=20)
plt.show()

plt.figure(figsize=(6, 4))
sns.scatterplot(data = data, x='social_media', y='total_bytes')
plt.title('Scatter plot showing relationship between total_bytes and social_media')
plt.show()

plt.figure(figsize=(6, 4))
# sns.jointplot(data = data, x='youtube', y='total_bytes', kind = 'hex', color = '#4CB391')
sns.scatterplot(data = data, x='youtube', y='total_bytes', color = '#4CB391')
# plt.title('Hexplot plot showing relationship between total_bytes and youtube')
plt.show()

plt.figure(figsize=(6, 4))
sns.scatterplot(data = data, x='gaming', y='total_bytes', color = 'violet')
plt.title('Scatter plot showing relationship between total_bytes and gaming')
plt.show()

plt.figure(figsize=(6, 4))
sns.scatterplot(data = data, x='netflix', y='total_bytes', color = 'grey')
plt.title('Scatter plot showing relationship between total_bytes and netflix')
plt.show()

plt.figure(figsize=(6, 4))
# sns.jointplot(data = data, x='other', y='total_bytes', kind = 'hex', color = '#4CB391')
sns.scatterplot(data = data, x='other', y='total_bytes', color = '#4CB391')
plt.title('scatter plot showing relationship between total_bytes and youtube')
plt.show()

"""## Variable transformations
– segment the users into top five decile classes based on the total duration for all sessions and compute the total data (DL+UL) per decile class.
"""

# adding Decile_rank column to the DataFrame 
data['Decile_rank'] = pd.qcut(data['Dur. (ms).1'], 5, labels = False) 
# data.columns

new_df = data[['Decile_rank', 'total_bytes']]
a = new_df.groupby('Decile_rank').sum()
a

plt.figure(figsize=(8,6))
sns.barplot(x = a.index, y = a.total_bytes)
plt.title('Total UL and DL byte per decile group')
plt.ylabel('total bytes')
plt.xlabel('decile rank')

"""## Correlation Analysis
– compute a correlation matrix for the following variables and interpret your findings: Social Media data, Google data, Email data, Youtube data, Netflix data, Gaming data, Other data - (jupyter notebook + slide )

### correlation on the total bytes
"""

corr_data = data[['social_media', 'google', 'email', 'youtube', 'netflix', 'gaming', 'other']]

    
corr = corr_data.corr()
f_fig, f_ax = plt.subplots(figsize=(8, 6))
f_ax = sns.heatmap(corr, vmin=0, vmax=1, square=True,
                   annot=False, annot_kws={"size": 10}, cmap="BuPu")

"""##### The correlation between the variables is super low.

### On the upload bytes
"""

corr_data = data[['Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']]

    
corr = corr_data.corr()
f_fig, f_ax = plt.subplots(figsize=(8, 6))
f_ax = sns.heatmap(corr, vmin=0, vmax=1, square=True,
                   annot=False, annot_kws={"size": 10}, cmap="BuPu")

"""##### The correlation between the variables is super low.

### On the download bytes
"""

corr_data = data[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']]

    
corr = corr_data.corr()
f_fig, f_ax = plt.subplots(figsize=(8, 6))
f_ax = sns.heatmap(corr, vmin=0, vmax=1, square=True,
                   annot=False, annot_kws={"size": 10}, cmap="BuPu")

"""##### The correlation between the variables is super low.

## Dimensionality Reduction
– perform a principal component analysis to reduce the dimensions of your data and provide a useful interpretation of the results.
"""

# standardize the data
from sklearn.preprocessing import StandardScaler


new_numerical = [] #this leaves out the categorical columns that were encoded but not replaced
for col in data.columns:
    if data[col].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:
        new_numerical.append(col)
        
seg = data[new_numerical].copy()
seg = seg.drop(['MSISDN/Number'], axis = 1) #drop the id col
scaler = StandardScaler()
scaled_data = scaler.fit_transform(seg)

scaled_data.shape

pca = PCA()
pca.fit(scaled_data)

pca.explained_variance_ratio_

plt.figure(figsize = (13,10))
plt.plot(range(64), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')
plt.xlabel('components')
plt.ylabel('cummulative explained variance')
plt.grid(True)

"""When choosing the number of components, the number cutting at least 80% of the cumm explained variance is considered.
**n = 23**
"""

pca = PCA(n_components = 23)
pca.fit(scaled_data)

pca_scores = pca.transform(scaled_data)

# new_data_df = pd.concat([seg.reset_index(drop = True), pd.DataFrame(pca_scores)], axis = 1)
# c.columns.values[-2:] = ['component1', 'component2']
# new_data_df.head() #contains the original cols 

a = pd.DataFrame(pca_scores)
a['MSISDN/Number'] = data['MSISDN/Number']
a.head()

#plotting the first 2 components since they explain most of the variability.
plt.figure(figsize=(12,10))
sns.scatterplot(x= a[0], y = a[1], hue = a['MSISDN/Number'])

"""## 2. User Engagement analysis
In the current dataset you’re expected to track the user’s engagement using the following engagement metrics: 
* sessions frequency 
* the duration of the session 
* the sessions total traffic (download and upload (bytes))
"""

# data.columns
# data['MSISDN/Number'].nunique()

"""### * Sessions frequency, sessions duration, sessions total traffic."""

user_data = data[['MSISDN/Number', 'total_bytes', 'Dur. (ms).1', 'Bearer Id']]

# def f(x):
#     d = {}
#     d['total_bytes'] = x['total_bytes'].sum()
#     d['Dur. (ms).1'] = x['Dur. (ms).1'].sum()
#     d['Bearer Id'] = x['Bearer Id'].count()
#     return pd.Series(d, index=['total_bytes', 'Dur. (ms).1', 'Bearer Id'])

# user_data.groupby('MSISDN/Number').apply(f)

agg_user_data = user_data.groupby('MSISDN/Number').agg({'Bearer Id':'count', 'Dur. (ms).1':'sum',  'total_bytes': 'sum'})
agg_user_data.columns = ['sessions_freq', 'sessions_duration(ms)', 'sessions_traffic(bytes)']
agg_user_data.head()

"""### Top 10 users with high sessions frequency"""

a = agg_user_data.sort_values(by = 'sessions_freq', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = a.index, y = a['sessions_freq'])
plt.ylabel('number of sessions')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest number of sessions')

"""### Top 10 users with high sessions traffic"""

b = agg_user_data.sort_values(by = 'sessions_traffic(bytes)', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = b.index, y = b['sessions_traffic(bytes)'])
plt.ylabel('total bytes')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest sessions traffic')

"""### Top 10 users with longest sessions duration"""

c = agg_user_data.sort_values(by = 'sessions_duration(ms)', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = b.index, y = b['sessions_duration(ms)'])
plt.ylabel('duration in ms')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with longest sessions duration')

# users in the 3 categories.
a = a.index.to_list()
b = b.index.to_list()
c = c.index.to_list()

def IntersecOfSets(arr1, arr2, arr3): 
    s1 = set(arr1) 
    s2 = set(arr2) 
    s3 = set(arr3) 

    set1 = s1.intersection(s2)
    result_set = set1.intersection(s3) 
      
    final_list = list(result_set) 
    print(final_list) 
  
IntersecOfSets(a, b, c)

"""### Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement."""

# Normalizing the aggregated dataset using log transformation
norm_agg_user_data = np.log1p(agg_user_data)
norm_agg_user_data.head()

norm_agg_user_data.shape

# kmeans on 3 clusters.
kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)
kmeans.fit(norm_agg_user_data)
kmeans_data = kmeans.predict(norm_agg_user_data)

# add the predicted clusters as a column in the df
norm_agg_user_data['clusters'] = pd.Series(kmeans_data, index=norm_agg_user_data.index)
norm_agg_user_data.head()

#plot the clusters(3D)
pd.plotting.parallel_coordinates(norm_agg_user_data, 'clusters')

# comparing the clusters in variable pairs

# frequency and duration
x = norm_agg_user_data[['sessions_freq', 'sessions_duration(ms)', 'clusters']]
plt.figure(figsize = (12,8))
sns.scatterplot(x = norm_agg_user_data['sessions_freq'],  y = norm_agg_user_data['sessions_duration(ms)'], palette = ['violet', 'red', 'green'], hue = norm_agg_user_data['clusters'])
# sns.palplot(sns.color_palette(flatui))
plt.title('Clusters of datapoints between the session freq and duration')
plt.xlabel('sessions frequency')
plt.ylabel('sessions duration')
plt.legend()

# comparing the clusters in variable pairs

# frequency and traffic
x = norm_agg_user_data[['sessions_freq', 'sessions_traffic(bytes)', 'clusters']]
plt.figure(figsize = (12,8))
sns.scatterplot(x = norm_agg_user_data['sessions_freq'],  y = norm_agg_user_data['sessions_traffic(bytes)'], palette = ['violet', 'red', 'green'], hue = norm_agg_user_data['clusters'])
# sns.palplot(sns.color_palette(flatui))
plt.title('Clusters of datapoints between the session freq and traffic')
plt.xlabel('sessions frequency')
plt.ylabel('sessions traffic')
plt.legend()

# comparing the clusters in variable pairs

# duration and traffic
x = norm_agg_user_data[['sessions_duration(ms)', 'sessions_traffic(bytes)', 'clusters']]
plt.figure(figsize = (12,8))
sns.scatterplot(x = norm_agg_user_data['sessions_duration(ms)'],  y = norm_agg_user_data['sessions_traffic(bytes)'], palette = ['violet', 'red', 'green'], hue = norm_agg_user_data['clusters'])
# sns.palplot(sns.color_palette(flatui))
plt.title('Clusters of datapoints between the session duration and traffic')
plt.xlabel('sessions duration')
plt.ylabel('sessions traffic')
plt.legend()

"""### * Compute the minimum, maximum, average & total non- normalized metrics for each cluster. Interpret your results visually with accompanying text."""

# add the clusters column to the non normalized data
agg_user_data['clusters'] = kmeans_data
agg_user_data.head()

agg_user_data.clusters.value_counts()

# cluster 1 (cluster = 0)
cluster_1 = agg_user_data.loc[agg_user_data['clusters'] == 0]   #get observations under cluster 1
# cluster_1.loc['total_per_metric'] = cluster_1.sum()             #get the totals per metric
print(cluster_1.shape)
cluster_1.tail()

cluster_1.describe() # get the min, max and average per metric in cluster 1

# cluster 2 (cluster = 1)
cluster_2 = agg_user_data.loc[agg_user_data['clusters'] == 1]   #get observations under cluster 2
# cluster_2.loc['total_per_metric'] = cluster_2.sum()             #get the totals per metric
print(cluster_2.shape)
cluster_2.tail()

cluster_2.describe() # get the min, max and average per metric in cluster 2

# cluster 3 (cluster = 2)
cluster_3 = agg_user_data.loc[agg_user_data['clusters'] == 2]   #get observations under cluster 3
# cluster_3.loc['total_per_metric'] = cluster_3.sum()             #get the totals per metric
print(cluster_3.shape)
cluster_3.tail()

cluster_3.describe() # get the min, max and average per metric in cluster 3

"""### * Aggregate user total traffic per application and derive the top 10 most engaged users per application"""

apps_traffic = data[['social_media', 'netflix', 'youtube', 'google', 'email', 'gaming', 'other', 'MSISDN/Number']]
apps_traffic.tail()

agg_apps_traffic = apps_traffic.groupby('MSISDN/Number').agg({'social_media':'sum', 'netflix':'sum', 'youtube':'sum', 'gaming':'sum', 
                                                            'google':'sum', 'email':'sum', 'other':'sum'})
agg_apps_traffic.head()

"""#### 10 users with highest social media traffic"""

b = agg_apps_traffic.sort_values(by = 'social_media', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = b.index, y = b['social_media'])
plt.ylabel('social_media total traffic')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest social_media traffic')

"""#### 10 users with highest netflix traffic"""

c = agg_apps_traffic.sort_values(by = 'netflix', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = c.index, y = c['netflix'])
plt.ylabel('netflix total traffic')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest netflix traffic')

"""#### 10 users with highest gaming traffic"""

d = agg_apps_traffic.sort_values(by = 'gaming', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = d.index, y = d['gaming'])
plt.ylabel('gaming total traffic')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest gaming traffic')

"""#### 10 users with highest google traffic"""

e = agg_apps_traffic.sort_values(by = 'google', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = e.index, y = e['google'])
plt.ylabel('google total traffic')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest google traffic')

"""#### 10 users with highest email traffic"""

f = agg_apps_traffic.sort_values(by = 'email', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = f.index, y = f['email'])
plt.ylabel('email total traffic')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest email traffic')

"""#### 10 users with highest youtube traffic"""

g = agg_apps_traffic.sort_values(by = 'youtube', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = g.index, y = g['youtube'])
plt.ylabel('youtube total traffic')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest youtube traffic')

"""#### 10 users with highest other traffic"""

h = agg_apps_traffic.sort_values(by = 'other', ascending = False).head(10)
plt.figure(figsize = (12,8))
sns.barplot(x = h.index, y = h['other'])
plt.ylabel('other total traffic')
plt.xlabel('user')
plt.xticks(rotation = 45)
plt.title('a barplot indicating 10 users with highest other traffic')

# users in the 3 categories.
b = b.index.to_list()
c = c.index.to_list()
d = d.index.to_list()
e = e.index.to_list()
f = f.index.to_list()
g = g.index.to_list()
h = h.index.to_list()


def IntersecOfSets(arr1, arr2, arr3, arr4, arr5, arr6, arr7): 
    s1 = set(arr1) 
    s2 = set(arr2) 
    s3 = set(arr3) 
    s4 = set(arr4)
    s5 = set(arr5)
    s6 = set(arr6)
    s7 = set(arr7)

    set1 = s1.intersection(s2)
    set2 = set1.intersection(s3) 
    set3 = set2.intersection(s4)
    set4 = set3.intersection(s5) 
    set5 = set4.intersection(s6) 
    set6 = set5.intersection(s7) 
      
    final_list = list(set6) 
    print(final_list) 
  
IntersecOfSets(b, c, d, e, f, g, h)

"""### * Plot the top 3 most used applications.
Most used applications can be determined by the Application with highest traffic data or the application with the most users.
"""

agg_apps_traffic.loc['total_app_traffic'] = agg_apps_traffic.sum()  
agg_apps_traffic.tail()

a = agg_apps_traffic.tail(1)
long_df = pd.melt(a)
long_df = long_df.sort_values(by = 'value', ascending = False).head(3)
plt.figure(figsize=(10,4))
sns.barplot(x = long_df.variable, y = long_df.value)
plt.title('top 3 most used apps based on total traffic')
plt.ylabel('total bytes')
plt.xlabel('application')

"""### * Using k-means clustering algorithm, group users in k engagement clusters based on the engagement metrics:"""

# the user engagement metrics df (already normalized due to different units of measurement)
norm_agg_user_data = norm_agg_user_data.drop(['clusters'], axis = 1)
norm_agg_user_data.head()

# determining number of clusters using elbow plot
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)
    kmeans.fit(norm_agg_user_data)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('wcss')
plt.show()

"""**k=4**"""

kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=0)
kmeans.fit(norm_agg_user_data)
y_means = kmeans.predict(norm_agg_user_data)
agg_user_data['clusters'] = y_means
# agg_user_data = agg_user_data.drop(['clusetrs'], axis = 1)
agg_user_data.head()

#clusters df
cluster_1 = agg_user_data[agg_user_data.clusters == 0]
cluster_2 = agg_user_data[agg_user_data.clusters == 1]
cluster_3 = agg_user_data[agg_user_data.clusters == 2]
cluster_4 = agg_user_data[agg_user_data.clusters == 3]

# descriptions
cluster_1.describe()

cluster_2.describe()

cluster_3.describe()

cluster_4.describe()

"""## 3. Experience Analytics

Aggregate, per customer, the following information (treat missing & outliers by replacing by the mean or the mode of the corresponding variable) -(jupyter notebook):
Average TCP retransmission
Average RTT
Handset type
Average throughput
"""

# data[numerical].columns

net_data = data[['MSISDN/Number', 'Handset Type', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',
                'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']]

net_data.head()

# null values are already treated, let's check the outliers using IQR score

#check if the exist
# RTT DL
Q1 = np.percentile(net_data['Avg RTT DL (ms)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg RTT DL (ms)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg RTT DL (ms)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg RTT DL (ms)'], interpolation = 'midpoint') 
o = (net_data['Avg RTT DL (ms)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg RTT DL (ms)'] > (Q3 + 1.5 * IQR))
m = o.unique()

#RTT UL
Q1 = np.percentile(net_data['Avg RTT UL (ms)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg RTT UL (ms)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg RTT UL (ms)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg RTT UL (ms)'], interpolation = 'midpoint') 
o = (net_data['Avg RTT UL (ms)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg RTT UL (ms)'] > (Q3 + 1.5 * IQR))
n = o.unique()

#TP DL
Q1 = np.percentile(net_data['Avg Bearer TP DL (kbps)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg Bearer TP DL (kbps)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg Bearer TP DL (kbps)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg Bearer TP DL (kbps)'], interpolation = 'midpoint') 
o = (net_data['Avg Bearer TP DL (kbps)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg Bearer TP DL (kbps)'] > (Q3 + 1.5 * IQR))
q = o.unique()

#TP UL
Q1 = np.percentile(net_data['Avg Bearer TP UL (kbps)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg Bearer TP UL (kbps)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg Bearer TP UL (kbps)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg Bearer TP UL (kbps)'], interpolation = 'midpoint') 
o = (net_data['Avg Bearer TP UL (kbps)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg Bearer TP UL (kbps)'] > (Q3 + 1.5 * IQR))
p = o.unique()

#TCP UL
Q1 = np.percentile(net_data['TCP UL Retrans. Vol (Bytes)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['TCP UL Retrans. Vol (Bytes)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['TCP UL Retrans. Vol (Bytes)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['TCP UL Retrans. Vol (Bytes)'], interpolation = 'midpoint') 
o = (net_data['TCP UL Retrans. Vol (Bytes)'] < (Q1 - 1.5 * IQR)) |(net_data['TCP UL Retrans. Vol (Bytes)'] > (Q3 + 1.5 * IQR))
q = o.unique()

#TCP DL
Q1 = np.percentile(net_data['TCP DL Retrans. Vol (Bytes)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['TCP DL Retrans. Vol (Bytes)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['TCP DL Retrans. Vol (Bytes)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['TCP DL Retrans. Vol (Bytes)'], interpolation = 'midpoint') 
o = (net_data['TCP DL Retrans. Vol (Bytes)'] < (Q1 - 1.5 * IQR)) |(net_data['TCP DL Retrans. Vol (Bytes)'] > (Q3 + 1.5 * IQR))
r = o.unique()

print('rtt dl', m)
print('rtt ul', n)
print('tp ul', p)
print('tp dl', q)
print('tcp dl', r)
print('tcp ul', q)

# o.shape
# net_data.shape

"""They all have outliers, now we have to treat them by replacing them with the low and high percentiles."""

net_data['Avg RTT DL (ms)'] = net_data['Avg RTT DL (ms)'].clip(lower=net_data['Avg RTT DL (ms)'].quantile(0.07), upper=net_data['Avg RTT DL (ms)'].quantile(0.93))
net_data['Avg RTT UL (ms)'] = net_data['Avg RTT UL (ms)'].clip(lower=net_data['Avg RTT UL (ms)'].quantile(0.10), upper=net_data['Avg RTT UL (ms)'].quantile(0.90))
net_data['Avg Bearer TP DL (kbps)'] = net_data['Avg Bearer TP DL (kbps)'].clip(lower=net_data['Avg Bearer TP DL (kbps)'].quantile(0.05), upper=net_data['Avg Bearer TP DL (kbps)'].quantile(0.95))
net_data['Avg Bearer TP UL (kbps)'] = net_data['Avg Bearer TP UL (kbps)'].clip(lower=net_data['Avg Bearer TP UL (kbps)'].quantile(0.20), upper=net_data['Avg Bearer TP UL (kbps)'].quantile(0.80))
net_data['TCP UL Retrans. Vol (Bytes)'] = net_data['TCP UL Retrans. Vol (Bytes)'].clip(lower=net_data['TCP UL Retrans. Vol (Bytes)'].quantile(0.05), upper=net_data['TCP UL Retrans. Vol (Bytes)'].quantile(0.95))
net_data['TCP DL Retrans. Vol (Bytes)'] = net_data['TCP DL Retrans. Vol (Bytes)'].clip(lower=net_data['TCP DL Retrans. Vol (Bytes)'].quantile(0.05), upper=net_data['TCP DL Retrans. Vol (Bytes)'].quantile(0.95))

# Confirming they've been treated
# RTT DL
Q1 = np.percentile(net_data['Avg RTT DL (ms)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg RTT DL (ms)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg RTT DL (ms)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg RTT DL (ms)'], interpolation = 'midpoint') 
o = (net_data['Avg RTT DL (ms)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg RTT DL (ms)'] > (Q3 + 1.5 * IQR))
m = o.unique()

#RTT UL
Q1 = np.percentile(net_data['Avg RTT UL (ms)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg RTT UL (ms)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg RTT UL (ms)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg RTT UL (ms)'], interpolation = 'midpoint') 
o = (net_data['Avg RTT UL (ms)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg RTT UL (ms)'] > (Q3 + 1.5 * IQR))
n = o.unique()

#TP DL
Q1 = np.percentile(net_data['Avg Bearer TP DL (kbps)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg Bearer TP DL (kbps)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg Bearer TP DL (kbps)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg Bearer TP DL (kbps)'], interpolation = 'midpoint') 
o = (net_data['Avg Bearer TP DL (kbps)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg Bearer TP DL (kbps)'] > (Q3 + 1.5 * IQR))
q = o.unique()

#TP UL
Q1 = np.percentile(net_data['Avg Bearer TP UL (kbps)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['Avg Bearer TP UL (kbps)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['Avg Bearer TP UL (kbps)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['Avg Bearer TP UL (kbps)'], interpolation = 'midpoint') 
o = (net_data['Avg Bearer TP UL (kbps)'] < (Q1 - 1.5 * IQR)) |(net_data['Avg Bearer TP UL (kbps)'] > (Q3 + 1.5 * IQR))
p = np.unique(o, return_counts = True)

#TCP UL
Q1 = np.percentile(net_data['TCP UL Retrans. Vol (Bytes)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['TCP UL Retrans. Vol (Bytes)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['TCP UL Retrans. Vol (Bytes)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['TCP UL Retrans. Vol (Bytes)'], interpolation = 'midpoint') 
o = (net_data['TCP UL Retrans. Vol (Bytes)'] < (Q1 - 1.5 * IQR)) |(net_data['TCP UL Retrans. Vol (Bytes)'] > (Q3 + 1.5 * IQR))
q = o.unique()

#TCP DL
Q1 = np.percentile(net_data['TCP DL Retrans. Vol (Bytes)'], 25, interpolation = 'midpoint')  
Q2 = np.percentile(net_data['TCP DL Retrans. Vol (Bytes)'], 50, interpolation = 'midpoint')  
Q3 = np.percentile(net_data['TCP DL Retrans. Vol (Bytes)'], 75, interpolation = 'midpoint')  
IQR = stats.iqr(net_data['TCP DL Retrans. Vol (Bytes)'], interpolation = 'midpoint') 
o = (net_data['TCP DL Retrans. Vol (Bytes)'] < (Q1 - 1.5 * IQR)) |(net_data['TCP DL Retrans. Vol (Bytes)'] > (Q3 + 1.5 * IQR))
r = o.unique()

print('rtt dl', m)
print('rtt ul', n)
print('tp ul', p)
print('tp dl', q)
print('tcp dl', r)
print('tcp ul', q)

"""All good now."""

# # replacing with mean
# mean = float(net_data['Avg Bearer TP UL (kbps)'].mean())
# lower=net_data['Avg Bearer TP UL (kbps)'].quantile(0.10)
# upper=net_data['Avg Bearer TP UL (kbps)'].quantile(0.90)
# net_data['Avg Bearer TP UL (kbps)'] = np.where((net_data['Avg Bearer TP UL (kbps)']  > upper), mean, net_data['Avg Bearer TP UL (kbps)'])
# net_data['Avg Bearer TP UL (kbps)'] = np.where((net_data['Avg Bearer TP UL (kbps)']  < lower), mean, net_data['Avg Bearer TP UL (kbps)'])

# generate total columns for rtt, tp and tcp
net_data['total_tp(kbps)'] = data['Avg Bearer TP DL (kbps)'] + data['Avg Bearer TP UL (kbps)']
net_data['total_rtt(ms)'] = data['Avg RTT DL (ms)'] + data['Avg RTT UL (ms)']
net_data['total_tcp(bytes)'] = data['TCP DL Retrans. Vol (Bytes)'] + data['TCP UL Retrans. Vol (Bytes)']
net_data.head()

# aggregating

# handset type
a = net_data[['MSISDN/Number', 'Handset Type']]
b = a.groupby('MSISDN/Number').count()
print(b.shape)
b = b.reset_index()
b.head()

# the others
c = net_data.drop(['Handset Type'], axis = 1)
d = c.groupby('MSISDN/Number').sum()
d = d.reset_index()
print(d.shape)
d.head()

# Merging them
agg_net_data = pd.merge(left=b, right=d, how='left', left_on='MSISDN/Number', right_on='MSISDN/Number')
print(agg_net_data.shape)
agg_net_data.head()

"""### Compute & list 10 of the top, bottom and most frequent:
* TCP values in the dataset. 
* RTT values in the dataset.
* Throughput values in the dataset.
"""

#tcp
tcp = data.sort_values(by = 'total_tcp', ascending = False)
top_tcp = tcp.total_tcp.head(10)
bottom_tcp = tcp.total_tcp.tail(10)
fre_tcp = data.total_tcp.value_counts(ascending = False).head(10)
print('top tcp\n', top_tcp.to_list())
print('bottom tcp\n', bottom_tcp.to_list())
print('freq tcp\n', fre_tcp.index.to_list())

#rtt
rtt = data.sort_values(by = 'total_rtt', ascending = False)
top_rtt= rtt.total_rtt.head(10)
bottom_rtt = rtt.total_rtt.tail(10)
fre_rtt = data.total_rtt.value_counts(ascending = False).head(10)
print('top rtt\n', top_rtt.to_list())
print('bottom rtt\n', bottom_rtt.to_list())
print('freq rtt\n', fre_rtt.index.to_list())

tp = data.sort_values(by = 'total_tp', ascending = False)
top_tp = tp.total_tp.head(10)
bottom_tp = tp.total_tp.tail(10)
fre_tp = data.total_tp.value_counts(ascending = False).head(10)
print('top tp\n', top_tp.to_list())
print('bottom tp\n', bottom_tp.to_list())
print('freq tp\n', fre_tp.index.to_list())

# user-based plots on the same

# tcp
a = agg_net_data.sort_values(by = 'total_tcp(bytes)', ascending = False).head(10)
b = agg_net_data.sort_values(by = 'total_tcp(bytes)', ascending = False).tail(10)
c = pd.DataFrame(agg_net_data['total_tcp(bytes)'].value_counts(ascending = False).head(10))

# top 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = a.index ,y = a['total_tcp(bytes)'], color="skyblue").set(title = 'Top 10 TCP values', xlabel = 'users')

# bottom 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = b.index ,y = b['total_tcp(bytes)'], color="violet").set(title = 'Bottom 10 TCP values', xlabel = 'users')

# Frequent 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = c.index ,y = c['total_tcp(bytes)'], color="violet").set(title = 'Frequent 10 TCP values', xlabel = 'users')

# rtt
a = agg_net_data.sort_values(by = 'total_rtt(ms)', ascending = False).head(10)
b = agg_net_data.sort_values(by = 'total_rtt(ms)', ascending = False).tail(10)
c = pd.DataFrame(agg_net_data['total_rtt(ms)'].value_counts(ascending = False).head(10))

# top 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = a.index ,y = a['total_rtt(ms)'], color="skyblue").set(title = 'Top 10 rtt values', xlabel = 'users')

# bottom 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = b.index ,y = b['total_rtt(ms)'], color="violet").set(title = 'Bottom 10 rtt values', xlabel = 'users')

# Frequent 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = c.index ,y = c['total_rtt(ms)'], color="pink").set(title = 'Frequent 10 rtt values', xlabel = 'users')

# tp
a = agg_net_data.sort_values(by = 'total_tp(kbps)', ascending = False).head(10)
b = agg_net_data.sort_values(by = 'total_tp(kbps)', ascending = False).tail(10)
c = pd.DataFrame(agg_net_data['total_tp(kbps)'].value_counts(ascending = False).head(10))

# top 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = a.index ,y = a['total_tp(kbps)'], color="skyblue").set(title = 'Top 10 tp values', xlabel = 'users')

# bottom 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = b.index ,y = b['total_tp(kbps)'], color="violet").set(title = 'Bottom 10 tp values', xlabel = 'users')

"""all zeros."""

# Frequent 10
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = c.index ,y = c['total_tp(kbps)'], color="pink").set(title = 'Frequent 10 tp values', xlabel = 'users')

"""### Compute & report:
* The distribution of the average throughput  per handset type and provide interpretation for your findings.
* The average TCP retransmission view per handset type and provide interpretation for your findings.
"""

data['total_tp(kbps)'] = data['Avg Bearer TP DL (kbps)'] + data['Avg Bearer TP UL (kbps)']
data['total_rtt(ms)'] = data['Avg RTT DL (ms)'] + data['Avg RTT UL (ms)']
data['total_tcp(bytes)'] = data['TCP DL Retrans. Vol (Bytes)'] + data['TCP UL Retrans. Vol (Bytes)']

handset_data = data[['Handset Type', 'total_tp(kbps)', 'total_tcp(bytes)']]
agg_handset_data = handset_data.groupby('Handset Type').mean()
agg_handset_data.columns = ['avg_tp_per_set', 'avg_tcp_per_set']
agg_handset_data.head()

# Average throughput/handset ~ distribution
sns.distplot(agg_handset_data['avg_tp_per_set'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(agg_handset_data['avg_tp_per_set'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('y distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(agg_handset_data['avg_tp_per_set'], plot=plt)
plt.show()

"""Heavily right tailed - Highly skewed."""

# The average TCP retransmission view per handset type and provide interpretation for your findings.
# aggregate function has already been applied 

# top 10 handsets per tcp average
a = agg_handset_data.sort_values(by = 'avg_tcp_per_set', ascending = False).head(10)

plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = a.index ,y = a['avg_tcp_per_set']).set(title = 'Top 10 handsets types per average tcp value', xlabel = 'handset type')

"""6 of the handset types with highest average tcp values are from the top 3 handset manufacturers."""

# bottom 10 handsets per tcp average
b = agg_handset_data.sort_values(by = 'avg_tcp_per_set', ascending = False).tail(10)

plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = b.index ,y = b['avg_tcp_per_set']).set(title = 'Bottom 10 handsets types per average tcp value', xlabel = 'handset type')

"""### Using the experience metrics above, perform a k-means clustering (where k = 3) to segment users into groups of experiences and provide a brief description of each cluster."""

cluster_data = agg_net_data[['Handset Type', 'total_tp(kbps)', 'total_rtt(ms)',  'total_tcp(bytes)']]
norm_cluster_data = np.log1p(cluster_data)   #log transformation
norm_cluster_data.head()

# kmeans on 3 clusters.
kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)
kmeans.fit(norm_cluster_data)
kmeans_data = kmeans.predict(norm_cluster_data)

# add the predicted clusters as a column in the df
cluster_data['clusters'] = pd.Series(kmeans_data, index=cluster_data.index)
cluster_data['clusters'] = pd.Series(kmeans_data, index=cluster_data.index)
cluster_data.head()

#clusters df
cluster_1 = cluster_data[cluster_data.clusters == 0]
cluster_2 = cluster_data[cluster_data.clusters == 1]
cluster_3 = cluster_data[cluster_data.clusters == 2]

# understand the clusters
cluster_1.describe()

cluster_2.describe()

cluster_3.describe()

"""## 4. Satisfaction Analysis

### Write a python program to assign:
* engagement score to each user. Consider the engagement score as the Euclidean distance between the user data point & the less engaged cluster (use the first clustering for this).
"""

# the results of the 1st clustering
norm_agg_user_data.head()

norm_agg_user_data.shape

# Identifying the least engaged cluster
norm_agg_user_data.clusters.value_counts()

"""Cluster 0 is the less engaged cluster, with the lowest count of 19362 data points.
* **engagement_score = euclidean_dist between a datapoint and cluster's 2 centroid.**
"""

data1 = norm_agg_user_data.drop(['clusters'], axis = 1)

km = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)
alldistances = km.fit_transform(data1) #returns euclidean distance btwn all datapoints and each cluster centroid

dist_df = pd.DataFrame(alldistances)
dist_2 = dist_df.iloc[:, 0].to_list() #pick out the centroid 2 distances
print(len(dist_2))
dist_2[:10]

# add the distances as engagement score column

norm_agg_user_data['engagement_score'] = dist_2
eng_scores_data = norm_agg_user_data.drop(['clusters'], axis = 1)
eng_scores_data.head()

"""* experience score to each user. Consider the experience score as the Euclidean distance between the user data point & the worst experience’s cluster."""

# picking out the 4 cols
exp_data = net_data.drop(['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 
                          'Avg Bearer TP UL (kbps)', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)'], axis = 1)
exp_data.head()

# groupby user
agg_exp_data = exp_data.groupby('MSISDN/Number').agg({'Handset Type':'count', 'total_tp(kbps)':'sum', 'total_rtt(ms)':'sum', 'total_tcp(bytes)':'sum'})
agg_exp_data.head()

# normalize and perform kmeans
norm_agg_exp_data = np.log1p(agg_exp_data)
norm_agg_exp_data.head()

# determining number of clusters using elbow plot
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)
    kmeans.fit(norm_agg_exp_data)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('wcss')
plt.show()

"""k = 3"""

# fit kmeans and identify least engaged cluster
k_means = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)
k_means.fit(norm_agg_exp_data)
clusters=k_means.predict(norm_agg_exp_data)

norm_agg_exp_data['clusters'] = clusters
norm_agg_exp_data.clusters.value_counts()

"""cluster 2 has the least.
*** exp_score = euclidean distance between datapoints and cluster 1's centroid**
"""

# fit kmeans
data1 = norm_agg_exp_data.drop(['clusters'], axis = 1)

km = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)
alldistances = km.fit_transform(data1) #returns euclidean distance btwn all datapoints to each cluster centroid

dist_df = pd.DataFrame(alldistances)
dist_0 = dist_df.iloc[:, 2].to_list() #pick out the centroid 0 distances
print(len(dist_0))
dist_0[:10]

# add the scores to the df
norm_agg_exp_data['experience_score'] = dist_0
exp_scores_data = norm_agg_exp_data.drop(['clusters'], axis = 1)
exp_scores_data.tail()

print(exp_scores_data.shape)
print(eng_scores_data.shape)

"""### Merge the engagement and experience datasets"""

exp_scores_data = exp_scores_data.reset_index()
eng_scores_data = eng_scores_data.reset_index()
avg_score_df = pd.merge(left = exp_scores_data, right = eng_scores_data, left_on = 'MSISDN/Number', right_on = 'MSISDN/Number')
print(avg_score_df.shape)
avg_score_df.tail()

#calculate the average score
avg_score_df['satisfaction_score'] = (avg_score_df['experience_score'] + avg_score_df['engagement_score'])/2
avg_score_df.head()

"""### Top 10 most satisfied users"""

a = avg_score_df.sort_values(by = 'satisfaction_score', ascending = False).head(10)
plt.figure(figsize = (12,8))
plt.xticks(rotation = 50)
sns.barplot(x = a.index ,y = a['satisfaction_score']).set(title = 'Top 10 Most satisfied users', xlabel = 'users')

"""### Run a regression model of your choice to predict the satisfaction score of a customer. 
Working with the 4 features from the average_score df
"""

all_variables = avg_score_df[['MSISDN/Number','engagement_score', 'experience_score', 'satisfaction_score']]
all_variables.head()

# # features transformations
# def logs(res, ls):
#     m = res.shape[1]
#     for l in ls:
#         res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   
#         res.columns.values[m] = l + '_log'
#         m += 1
#     return res

# log_features = ['Handset Type', 'total_tp(kbps)', 'total_rtt(ms)','total_tcp(bytes)', 'experience_score', 'sessions_freq',
#                 'sessions_duration(ms)', 'sessions_traffic(bytes)', 'engagement_score','satisfaction_score']

# all_variables = logs(avg_score_df, log_features)
# all_variables.head(2)

# def squares(res, ls):
#     m = res.shape[1]
#     for l in ls:
#         res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   
#         res.columns.values[m] = l + '_sq'
#         m += 1
#     return res 

# squared_features = ['Handset Type', 'total_tp(kbps)', 'total_rtt(ms)','total_tcp(bytes)', 'experience_score', 'sessions_freq',
#                     'sessions_duration(ms)', 'sessions_traffic(bytes)', 'engagement_score','satisfaction_score']

# all_variables = squares(all_variables, squared_features)
# print(all_variables.shape)
# all_variables.head(2)

# split  dataset into train and test sets
from sklearn.model_selection import train_test_split

x = all_variables.drop(['satisfaction_score'], axis = 1)
y = all_variables[['satisfaction_score']]
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = .3, random_state = 0)
print('xtrain shape:', x_train.shape)
print('xtest shape:', x_test.shape)
print('ytrain shape:', y_train.shape)
print('ytest shape:', y_test.shape)

# drop the user column from the xtrain. Save it in another df from the xtest set
x_train = x_train.drop(['MSISDN/Number'], axis = 1)
test_user = x_test[['MSISDN/Number']]
x_test = x_test.drop(['MSISDN/Number'], axis = 1)
print('xtrain shape:', x_train.shape)
print('xtest shape:', x_test.shape)
print('users df shape:', test_user.shape)

# Models hyperparameters tuning
# I intend to observe the performance of 3 models (2 boosters and one ensemble method) then use the one with the lowest MSE score to predict the x_test set.

# Random Forest
regressor = RandomForestRegressor(n_estimators=300, random_state=0)


#gradboost
gdb = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10, 
                                   loss='huber', random_state =0)


#xgboost
xgb = XGBRegressor(colsample_bytree=0.4603, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=0,
                             random_state =0, nthread = -1)

# # fitting and observing performannce through

# # score
# n_folds = 5
# def rmsle_cv(model):
#     kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)
#     rmse= np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring="neg_mean_squared_error", cv = kf))
#     return(rmse)

# score = rmsle_cv(xgb)
# print("\nExtreme grad boostscore: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

# score = rmsle_cv(regressor)
# print("\nRandom Forest score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

# score = rmsle_cv(gdb)
# print("\nGrad Boost score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

# l = [0.0012, 0.0000, 0.0037]
# c = ['xgb', 'random forest', 'gradboost']
# df = pd.DataFrame(l, index = c)
# sns.barplot(x = df.index, y = df[0])

# rf prediction
regressor.fit(x_train.values, y_train)
rf_pred = regressor.predict(x_test.values)

# xgb prediction
xgb.fit(x_train.values, y_train)
xgb_pred = xgb.predict(x_test.values)

# gdb prediction
gdb.fit(x_train.values, y_train)
gdb_pred = gdb.predict(x_test.values)

from sklearn.metrics import mean_absolute_error, mean_squared_error

print("Mean absolute error for random forest on test data =", mean_absolute_error(y_test, rf_pred))
print("Mean absolute error for xgboost on test data=", mean_absolute_error(y_test, xgb_pred))
print("Mean absolute error for gradboost on test data=", mean_absolute_error(y_test, gdb_pred))

print("\nMean squared error for random forest on test data =", mean_squared_error(y_test, rf_pred))
print("Mean squared error for xgboost on test data=", mean_squared_error(y_test, xgb_pred))
print("Mean squared error for gradboost on test data=", mean_squared_error(y_test, gdb_pred))

"""Working with the randomforest predictions since it has the smallest mean squared error."""

# save predictions.
predictions = pd.DataFrame()
predictions['users'] = test_user['MSISDN/Number']
predictions['satisfaction_score'] = rf_pred
print(predictions.shape)
predictions.head()

"""### Run a k-means (k=2) on the engagement & the experience score."""

ds = avg_score_df[['MSISDN/Number', 'engagement_score', 'experience_score']]

# no need for standardization (same unit if measurement)
k_means = KMeans(n_clusters = 2, init = 'k-means++', random_state = 0)
k_means.fit(ds)
clusters=k_means.predict(ds)

ds['clusters'] = clusters
ds.tail()

"""### Aggregate the average engagement & experience score per cluster."""

cluster_agg = ds.groupby('clusters').agg({'experience_score':'mean', 'engagement_score':'mean'})
cluster_agg

"""###  Export your final table containing all user id + engagement, experience & satisfaction scores in your local MySQL database. Report a screenshot of a select on the exported table."""

!pip install mysql-connector

!pip install pymysql

data['MSISDN/Number'].nunique()

df = ds[['MSISDN/Number', 'engagement_score', 'experience_score', 'satisfaction_score']]
df.to_csv('finaldf.csv')
print(df.shape)
df.head(2)

from pandas.io import sql
import mysql.connector
import pymysql


dsn_database = "ada"  
dsn_hostname = "localhost"      
dsn_port = 3306              
dsn_uid = "user1"         
dsn_pwd = "******"

con = mysql.connector.connect(host=dsn_hostname, port=dsn_port, user=dsn_uid, passwd=dsn_pwd, db=dsn_database)
df.to_sql(con=con, name='final_telecomm_table', if_exists='replace', flavor='mysql')

"""[Back to top](#Table-of-Contents)"""

